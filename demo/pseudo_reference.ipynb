{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating a pseudo reference database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading libraries\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import bokeh\n",
    "import bokeh.plotting\n",
    "\n",
    "# For interfacing with the file system\n",
    "import glob\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "\n",
    "import importlib\n",
    "import bilge_pype as bpy\n",
    "\n",
    "# pipes bokeh output to the notebook\n",
    "bokeh.io.output_notebook()\n",
    "# enables some logging output\n",
    "bpy.init_log(level='INFO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data into python\n",
    "After running the basecaller on fast5 files, you will have a folder containing basecalled fastq reads. `bilge_pype.py` contains a few useful functions for reading and writing fastq and fasta files. As some of these files could be quite large, we use the `low_mem=True` option to save on RAM when running this notebook, but still allow us to fetch the sequence and quality strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_folder = 'fastq'\n",
    "files = glob.glob(fastq_folder+'/*.fastq')         # find list of fastq files to read\n",
    "df_reads = bpy.load_ONT_fastq(files, low_mem=True) # reads fastq files and parses the header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reads.iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`id` is a unique identifier string for each fastq read.\n",
    "\n",
    "`ch` is the channel where the read originated from.\n",
    "\n",
    "`start_time` is the acquisition time of read.\n",
    "\n",
    "`length` is the length of the sequence.\n",
    "\n",
    "Apply `add_seq()` to `df_reads` to get the sequence and quality information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpy.add_seq(df_reads.iloc[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for reference sequences\n",
    "Consensus building in `ashure.py` works by using a reference sequence to search for repeated sequences of interest in PCR products generated by rolling circle amplification. These repeats are later aligned to generate a more error free consensus. In the absence of a reference database, we would need a best guess at what we are sequencing. This is our pseudo reference database, which can be generated with primer information. We just need to know what sequences are flanking our mystery gene.\n",
    "\n",
    "Searching every read for forward and reverse primer pairs is time intensive and unnecessary. PCR amplifies the original templates, making the reference over represented. We can filter out large repetitive fragments and just examine a subset that are likely to contain our reference sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_length_dist(ax, df, color = 'red', alpha = 0.5, label = '', by_counts = True):\n",
    "    if by_counts:\n",
    "        plt.hist(df['length'], bins = np.arange(0,50e3,100),\n",
    "                 alpha = alpha, color = color, label = label)\n",
    "        plt.ylabel('counts')\n",
    "        plt.yscale('symlog')\n",
    "    else:\n",
    "        plt.hist(df['length'], bins = np.arange(0,50e3,100),\n",
    "                 weights = df['length']/1e6, alpha = alpha, color = color, label = label)\n",
    "        plt.ylabel('MB (nt)')\n",
    "    plt.xlabel('read length (nt)')\n",
    "    plt.xlim([0,50e3])\n",
    "\n",
    "# print the distribution of read lengths\n",
    "plt.figure(1, figsize=(10,3))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plot_length_dist(plt.gca(), df_reads, color='red')\n",
    "plt.subplot(1,2,2)\n",
    "plot_length_dist(plt.gca(), df_reads, color='red', by_counts=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cytochrome oxidase subunit 1 (CO1) gene is about 700bp. Size filtering is used to filter out concatemers. We want to focus on non-repeated reads because there is a chance some reads are so corrupted that the primer sequence could not be found in the correct location. You could end up with concatemers instead of a single full length CO1.\n",
    "\n",
    "To illustrate this, lets run `ashure.py` with length filtering from 500-1200bp and 500-3000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = './ashure.py prfg -fq fastq/*.fastq -fs 500-1200 -p primers.csv -o pseudo_ref.csv.gz --low_mem -r'\n",
    "subprocess.run(cmd, shell=True) # submit the command to shell\n",
    "cmd = './ashure.py prfg -fq fastq/*.fastq -fs 500-3000 -p primers.csv -o pseudo_ref_bad.csv.gz --low_mem -r'\n",
    "subprocess.run(cmd, shell=True) # submit the command to shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pseudo reference sequences\n",
    "df_prf1 = pd.read_csv('pseudo_ref.csv.gz')\n",
    "df_prf2 = pd.read_csv('pseudo_ref_bad.csv.gz')\n",
    "# add length information\n",
    "df_prf1['length'] = [len(i) for i in df_prf1['sequence']]\n",
    "df_prf2['length'] = [len(i) for i in df_prf2['sequence']]\n",
    "# plot the length distribution\n",
    "plt.figure(1, figsize=(10,3))\n",
    "plt.hist(df_prf2['length'], bins=np.arange(0,3000,10), color='red', alpha=0.5, label='500-3000bp')\n",
    "plt.hist(df_prf1['length'], bins=np.arange(0,3000,10), color='blue', alpha=0.5, label='500-1200bp')\n",
    "plt.legend()\n",
    "plt.xlabel('sequence length (bp)')\n",
    "plt.ylabel('frequency (count)')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With larger bp size windows, concatemers will show up as peaks at around 1500bp and 2400bp. These concatemers are sometimes undesirable because they interfer with proper dereplication of an RCA read. In special cases, concatemers could be desirable because the gene of interest has many repetitive domains such as ankyrin proteins. We leave these decisions up to the user as you are the expert on the gene you are sequencing.\n",
    "\n",
    "Filtering and concatenation of reads by size is fairly straight forward. An example is provided below.\n",
    "```python\n",
    "x = df_prf2[(df_prf2['length'] > 700) & (df_prf2['length'] < 1000)]    # get sequences between 700-1000bp\n",
    "y = df_prf2[(df_prf2['length'] > 1200) & (df_prf2['length'] < 1600)]   # get sequences between 1200-1600bp\n",
    "df = pd.concat([x,y]) # concatenate the dateframes\n",
    "df.to_csv('example.csv.gz', compression='infer', index=False)    # write the data to csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering and compressing the pseudo reference\n",
    "Compacting and dereplicating the pseudo reference can significantly speed up run time without hurting accuracy and sensitivity. This is possible because the pseudo reference contains many redundant sequences. Remember that exact matches are not required. The pseudo reference is meant to be a fuzzy representation of the overall dataset such that genes of interest can quickly be found.\n",
    "\n",
    "The code snippet below shows how to align the pseudo reference against the true reference database for mock50 samples and reveal the redundancy of information in the pseudo reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load true reference and pseudo reference databases\n",
    "df_ref = bpy.read_fasta('ref_db.fa')\n",
    "df_prf = pd.read_csv('pseudo_ref.csv.gz')\n",
    "# align pseudo reference against the mock50\n",
    "df_acc = bpy.run_minimap2(df_prf, df_ref, config='-k8 -w1', cleanup=True)\n",
    "df_acc = bpy.get_best(df_acc,['query_id'], metric='AS', stat='idxmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`run_minimap2()` is a wrapper function to minimap2 in `bilge_pype.py`. To get more help, type `help(bpy.run_minimap2)` in python shell.\n",
    "\n",
    "`get_best()` is a helpful function to filter to the best alignment\n",
    "\n",
    "Alignment of fasta or csv files against other fasta or csv file can also be invoked using `get_accuracy.py`. This script depends on `bilge_pype.py`. Below is an example usage:\n",
    "\n",
    "```bash\n",
    "./get_accuracy.py -h     # get help\n",
    "./get_accuracy.py -i pseudo_ref.csv.gz -o acc.csv.gz   # input pseudo_ref.csv.gz and output to acc.csv.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc(df_acc, haps):\n",
    "    data = df_acc[df_acc['database_id'].isin(haps)]\n",
    "    data = data.sort_values(by = ['database_id'])\n",
    "    data['error'] = 1-data['similarity']\n",
    "\n",
    "    # subsample to data to make swarmplot work\n",
    "    df_sub = bpy.stats_subsample(data, col='database_id', N=100)\n",
    "    # split into subplots swarm plot don't look crowded\n",
    "    slist = np.array_split(haps,2)\n",
    "    for i in range(0, len(slist)):\n",
    "        plt.subplot(1,2,i+1)\n",
    "        d = data[data['database_id'].isin(slist[i])]\n",
    "        cmap = plt.cm.get_cmap('Dark2').colors\n",
    "        bpy.mpl_box_whisker(plt.gca(), d[['database_id','error']],text_y=0.9, counts=True, cmap=cmap)\n",
    "        bpy.mpl_violin(plt.gca(), d[['database_id','error']], face_color='white', edge_color='black')\n",
    "        d = df_sub[df_sub['database_id'].isin(slist[i])]\n",
    "        sns.swarmplot(x='error', y='database_id', data=d, alpha=1, color='Black', s=2)\n",
    "        plt.ylabel('')\n",
    "        if i == 0: plt.ylabel('Haplotypes')\n",
    "        plt.xlim([0,1])\n",
    "    plt.tight_layout()\n",
    "\n",
    "# get the names of the mock50 haplotypes\n",
    "haps = df_ref[['HAP' in i for i in df_ref['id']]]['id'].values\n",
    "# make a violin/swarm plot\n",
    "plt.figure(1, figsize=(10,5))\n",
    "plot_acc(df_acc, haps)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The violin/swarm plot above shows that some haplotypes are represented by 1 sequence, while some are represented by several hundred sequences. Compute time is waste on aligning every fastq read to every closely related pseudo reference sequence.\n",
    "\n",
    "The ideal pseudo reference database for this dataset is around 50 sequences or one sequence for each mock50 sample. The pseudo reference can be compacted by invoking the `clst` module in `ashure.py`. The `clst` module uses OPTICS density clustering to order, label, merge, and return the representative center sequences for each haplotype. OPTICS is an unsupervised clustering algorithm that uses density to flag potential sequence clusters and cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = './ashure.py clst -i pseudo_ref.csv.gz -o pseudo_clst.csv.gz -iter 10 -r'  # runs clustering for 10 iterations\n",
    "subprocess.run(cmd, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df_prfc = pd.read_csv('pseudo_clst.csv.gz')\n",
    "df_prf = pd.read_csv('pseudo_ref.csv.gz')\n",
    "df_prf['length'] = [len(i) for i in df_prf['sequence']]\n",
    "df_prfc['length'] = [len(i) for i in df_prfc['sequence']]\n",
    "# align pseudo reference against the mock50\n",
    "df_acc2 = bpy.run_minimap2(df_prfc, df_ref, config='-k8 -w1', cleanup=True)\n",
    "df_acc2 = bpy.get_best(df_acc2,['query_id'], metric='AS', stat='idxmax')\n",
    "plt.figure(1, figsize=(10,5))\n",
    "plot_acc(df_acc2, haps)\n",
    "\n",
    "plt.figure(2, figsize=(12,3))\n",
    "plt.hist(df_prf['length'], bins=np.arange(0,3000,10), color='red', label='unclustered')\n",
    "plt.hist(df_prfc['length'], bins=np.arange(0,3000,10), color='blue', label='cluster centers')\n",
    "plt.legend()\n",
    "plt.xlabel('sequence length (bp)')\n",
    "plt.ylabel('frequency (count)')\n",
    "plt.yscale('symlog')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('clst module produced ',len(df_prfc),' cluster centers')\n",
    "print('original pseudo reference size = ',len(df_prf))\n",
    "n1 = len(np.unique(df_acc[df_acc['database_id'].isin(haps)]['database_id']))\n",
    "n2 = len(np.unique(df_acc2[df_acc2['database_id'].isin(haps)]['database_id']))\n",
    "print('haplotypes before: ', n1)\n",
    "print('haplotypes after: ', n2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering removed many of the redundant sequences. Although some rare haplotypes may have been lost, we retain the majority of the interesting sequences which could still be in our sample.\n",
    "\n",
    "For some sequences, the accuracy of the pseudo reference entry improves because merging via multi-alignment successfully canceled out sequencing errors. For other sequences, the accuracy seems to have decreased because wrong sequences were merged together.\n",
    "\n",
    "More on this will be elaborated in the next section and in [clustering.ipynb](clustering.ipynb)\n",
    "\n",
    "To run the remainder of `ashure.py` with this compressed pseudo reference database, an example is shown below:\n",
    "\n",
    "```bash\n",
    "./ashure.py -h                  # get help\n",
    "./ashure.py run -fq fastq/* -p primers.csv -db pseudo_clst.csv.gz -o1 cons.csv.gz --low_mem  # run the whole pipeline\n",
    "```\n",
    "\n",
    "### Detection of rare haplotypes\n",
    "If detection of rare haplotypes is a priority, do not worry. The pseudo reference database does not need to be perfect. The sequences only need to approximately match your gene of interest. If the rare gene is wholly missed in the first run, `ashure.py` can be rerun on reads which did not strongly map to anything.\n",
    "\n",
    "More on how to filter and rerun `ashure.py` to find rare haplotypes will be covered in [rare_haplotypes.ipynb](rare_haplotypes.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the pseudo reference clusters\n",
    "\n",
    "The similarity relationships between sequences can be visualized via a dendrogram, pairwise matrix, or tsne plot. In this section, we show how functions in `bilge_pype.py` can help you generate pairwise distance data, plot a dendrogram, and make tsne plots to visualize your sequence data.\n",
    "\n",
    "### Getting pairwise distance information\n",
    "In the code snippet below, pseudo reference sequences, cluster center sequences of the pseudo reference, and the mock50 reference sequences are loaded and concatenated. Pairwise alignments are computed via `run_minimap2()` and saved to a csv file called `pseudo_pw.csv.gz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pw(df):\n",
    "    # note that -D in config remove entries for diagonal alignments. You will need to add this back for similarity measures\n",
    "    df_pw = bpy.run_minimap2(df, df, config='-k15 -w10 -p 0.9 -D -dual=no', cleanup=True)\n",
    "    return bpy.get_best(df_pw,['query_id','database_id'],metric='AS', stat='idxmax')  # get the best alignment\n",
    "\n",
    "# load data and concatenate dataframes\n",
    "df_prf = pd.read_csv('pseudo_ref.csv.gz')\n",
    "df_cl = pd.read_csv('pseudo_clst.csv.gz')\n",
    "df_ref = bpy.read_fasta('ref_db.fa')\n",
    "df = pd.concat([df_prf, df_cl, df_ref])\n",
    "df_pw = compute_pw(df[['id','sequence']])\n",
    "df_pw.to_csv('pseudo_pw.csv.gz', index=False, compression='infer') # save the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting a dendrogram\n",
    "The code snippet below shows how to compute and plot a dendogram. Dendrograms show the relationship between pairs of sequences or subclusters. The example below, [complete linkage](https://en.wikipedia.org/wiki/Complete-linkage_clustering) clustering, tries to group together pairs of related sequences or subclusters while minimizing the maximum distance within a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the TSNE representation of the distance relationships\n",
    "Dendrograms are not the ideal way to view evolutionary relationships. Complicated relationships such as evenly spaced points on a sphere are not captured by dendrograms. Neighborhood embeddings such as TSNE are better at preserving high dimensional relationships. TSNE projects high dimensional relations onto low dimensional space and tries to minimize the divergence in neighborhood structure between the low dimension and high dimension representations of the data.\n",
    "\n",
    "The code snippet below shows how to apply TSNE to the pseudo reference and obtain a 2D representation of the relationships between sequences. HDBSCAN is used to flag clusters in 2D space. Data is returned as a pandas dataframe and saved to `pseudo_tnse.csv.gz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster(dist, metric='precomputed'):\n",
    "    # runs tsne on distance matrix\n",
    "    tsne = bpy.run_TSNE(dist, metric=metric)\n",
    "    # run hdbscan on tsne output\n",
    "    hdb = bpy.cluster_HDBSCAN(tsne, metric='euclidean', min_samples=20, min_cluster_size=10)\n",
    "    hdb = hdb.rename(columns={'cluster_id':'hdbscan_id','ordering':'hdbscan_order'})\n",
    "    return tsne.merge(hdb, on='id', how='left')\n",
    "\n",
    "def get_dist_matrix(df):\n",
    "    # extract the pairwise distance matrix and use match_score as the metric\n",
    "    # match score is the tot_matching_bp/seq_len\n",
    "    m = bpy.get_feature_vector(df[['query_id','database_id','similarity']], symmetric=True)\n",
    "    # make this matrix symmetric\n",
    "    m = bpy.get_symmetric_matrix(m, sym_larger=False)\n",
    "    # invert similarity to distance\n",
    "    for i in range(0,len(m)):\n",
    "        m.iloc[:,i] = 1-m.iloc[:,i]\n",
    "        m.iat[i,i] = 0 # set diagonal values to zero\n",
    "    return m\n",
    "\n",
    "# load pairwise data\n",
    "df = pd.read_csv('pseudo_pw.csv.gz')\n",
    "# get the distance matrix\n",
    "m = get_dist_matrix(df)\n",
    "# perform tsne, hdscan, and optics on the data\n",
    "tsne = get_cluster(m, metric='precomputed')\n",
    "tsne.to_csv('pseudo_tsne.csv.gz', compression='infer', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet below aligns the pseudo reference against the pseudo cluster centers and true reference. This information lets us colorize the scatter plots and interpret how well OPTICS performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sequence data\n",
    "prf = pd.read_csv('pseudo_ref.csv.gz')\n",
    "prf['source'] = 'samples'\n",
    "ref = bpy.read_fasta('ref_db.fa')\n",
    "ref = ref[[('HAP' in i) for i in ref['id']]]\n",
    "ref['source'] = 'reference'\n",
    "ref['ref_id'] = ref['id']\n",
    "clst = pd.read_csv('pseudo_clst.csv.gz')\n",
    "clst['source'] = 'clst'\n",
    "clst['clst_id'] = clst['id']\n",
    "# align reference database to pseudo reference and pseudo reference clusters to pseudo reference\n",
    "A = bpy.run_minimap2(prf, ref, config='-k8 -w1', cleanup=True).rename(columns={'query_id':'id'})\n",
    "A = bpy.get_best(A,['id'],metric='match_score',stat='idxmax')\n",
    "B = bpy.run_minimap2(prf, clst, config='-k8 -w1', cleanup=True).rename(columns={'query_id':'id'})\n",
    "B = bpy.get_best(B,['id'],metric='match_score',stat='idxmax')\n",
    "A = A.rename(columns={'database_id':'ref_id','match_score':'ref_match'})\n",
    "B = B.rename(columns={'database_id':'clst_id','match_score':'clst_match'})\n",
    "# merge alignment data\n",
    "prf = prf.merge(A[['id','ref_id','ref_match']], on='id', how='left')\n",
    "prf = prf.merge(B[['id','clst_id','clst_match']], on='id', how='left')\n",
    "df = pd.concat([prf,ref,clst])\n",
    "# merge into tsne\n",
    "tsne = pd.read_csv('pseudo_tsne.csv.gz')\n",
    "tsne = tsne.merge(df, on='id', how='left')\n",
    "# add colors to ref_id\n",
    "col = 'ref_id'\n",
    "rid = np.unique(tsne[col].dropna())\n",
    "cmap = bokeh.palettes.Category20b_20\n",
    "colors = [cmap[i%len(cmap)] for i in range(0,len(rid))]\n",
    "colors = pd.DataFrame(np.transpose([rid,colors]), columns=[col,'ref_id_color'])\n",
    "tsne = tsne.merge(colors,on='ref_id',how='left')\n",
    "# add colors to clst_id\n",
    "col = 'clst_id'\n",
    "rid = np.unique(tsne[col].dropna())\n",
    "colors = [cmap[i%len(cmap)] for i in range(0,len(rid))]\n",
    "colors = pd.DataFrame(np.transpose([rid,colors]), columns=[col,'clst_id_color'])\n",
    "tsne = tsne.merge(colors,on='clst_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOLTIPS = [('id','@id'),\n",
    "            ('ref_id', '@ref_id'),\n",
    "            ('ref_match','@ref_match'),\n",
    "            ('clst_id','@clst_id'),\n",
    "            ('clst_match','@clst_match'),\n",
    "            ('hdbscan_id','@hdbscan_id'),\n",
    "            ('optics_id','@optics_id')]\n",
    "p = bokeh.plotting.figure(plot_width=600, plot_height=500, tooltips=TOOLTIPS)\n",
    "\n",
    "# plot samples colored by ref_match\n",
    "d = tsne[tsne['source']=='samples']\n",
    "mapper = bokeh.transform.linear_cmap(field_name='ref_match', palette=bokeh.palettes.Spectral11, low=0.8, high=1)\n",
    "color_bar = bokeh.models.ColorBar(color_mapper=mapper['transform'], width=8,  location=(0,0))\n",
    "#p.circle('f_0', 'f_1', color=mapper, size=5, source=d)\n",
    "p.circle('f_0', 'f_1', color='clst_id_color', size=5, source=d)\n",
    "\n",
    "# plot cluster centers\n",
    "d = tsne[tsne['source']=='clst']\n",
    "p.square('f_0', 'f_1', line_color='black', fill_color='red', size=6, source=d)\n",
    "d = tsne[tsne['source']=='reference']\n",
    "p.diamond('f_0', 'f_1', line_color='black', fill_color='blue', size=10, source=d)\n",
    "\n",
    "p.add_layout(color_bar, 'right')\n",
    "bokeh.plotting.show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HAP61 is on edge of HAP38\n",
    "\n",
    "HAP05, HAP54 is on edge of HAP39\n",
    "\n",
    "HAP31 on edge of HAP28\n",
    "\n",
    "HAP18 is on edge of HAP62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
